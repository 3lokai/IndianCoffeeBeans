# â˜• Indian Coffee Beans Scraper â€“ Final PRD (v3)

**Last Updated:** May 2025
**Owner:** GT
**Goal:** Build and maintain a robust, scalable scraper to extract and enrich Indian coffee roaster product data and feed it into Supabase.

---

## ğŸ¯ Objectives

* Crawl and extract coffee product data from \~50 Indian roaster websites.
* Support all platforms (Shopify, WooCommerce, generic).
* Prioritize structured extraction using Crawl4AI.
* Use DeepSeek as intelligent fallback for missing attributes.
* Maintain a unified, cache-aware, async pipeline.
* Enable batch upserts to Supabase + optional CSV exports.

---

## ğŸ—‚ï¸ Folder Structure (Confirmed)

```
indian_coffee_beans_scraper/
â”œâ”€â”€ main.py
â”œâ”€â”€ config.py
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt

â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ roaster_schema.py
â”‚   â””â”€â”€ product_schemas.py

â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ roaster_pipeline.py
â”‚   â”œâ”€â”€ platform_detector.py
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ json_css_extractor.py
â”‚   â”‚   â””â”€â”€ deepseek_extractor.py
â”‚   â””â”€â”€ discoverers/
â”‚       â”œâ”€â”€ discovery_manager.py
â”‚       â”œâ”€â”€ sitemap_discoverer.py
â”‚       â”œâ”€â”€ html_discoverer.py
â”‚       â””â”€â”€ structured_data_discoverer.py

â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ cache.py

â”œâ”€â”€ db/
â”‚   â””â”€â”€ supabase_client.py

â””â”€â”€ data/
    â”œâ”€â”€ cache/
    â””â”€â”€ output/
```

---

## ğŸ”§ Pipeline Flow

1. **Input:** CSV of roaster info (`name, website`)
2. **Roaster Metadata Extraction:**

   * Crawl homepage â†’ extract `logo`, `description`, `platform`
   * Store in Supabase (`roasters`)
3. **Product Discovery:**
   Try in this order:

   * `/products.json` or known API endpoints
   * `sitemap.xml` parsing
   * HTML crawling for internal product links
   * Structured data tags (e.g. @graph, OpenGraph)
4. **Product Extraction:**

   * JSON-CSS schema with Crawl4AI
   * If â‰¥2 fields are missing â†’ run `fit_markdown` through DeepSeek
5. **Data Validation + Upload:**

   * Use Pydantic models
   * Batch upsert to Supabase
   * Save CSV copy (optional)
6. **Caching + Error Logging:**

   * Store cache per product + roaster
   * Skip URLs already processed unless `--refresh` is passed

---

## ğŸ§± Modules to Build (with Priority)

### ğŸŸ© Phase 1: Base Setup (Must-Have)

| Priority | Module          | File                    | Purpose                       |
| -------- | --------------- | ----------------------- | ----------------------------- |
| âœ…        | Config loader   | `config.py`             | Loads `.env` and constants    |
| âœ…        | Supabase client | `db/supabase_client.py` | Insert/upsert to Supabase     |
| âœ…        | Models          | `common/models.py`      | Pydantic data validation      |
| âœ…        | Utilities       | `common/utils.py`       | Slugging, price parsing, etc. |

### ğŸŸ¨ Phase 2: Discovery & Roaster Metadata

| Priority | Module                | File                            | Purpose                   |
| -------- | --------------------- | ------------------------------- | ------------------------- |
| ğŸ”¼       | Roaster crawler       | `roaster_pipeline.py`           | Crawl homepage + schema   |
| ğŸ”¼       | Platform detector     | `platform_detector.py`          | Heuristic detection logic |
| ğŸ”¼       | Discovery manager     | `discovery_manager.py`          | Orchestrates discoverers  |
| ğŸŸ¡       | Sitemap discoverer    | `sitemap_discoverer.py`         | Crawl sitemap URLs        |
| ğŸŸ¡       | HTML discoverer       | `html_discoverer.py`            | Shallow crawl for links   |
| ğŸŸ¡       | Structured discoverer | `structured_data_discoverer.py` | Use `@graph` + OpenGraph  |

### ğŸŸ§ Phase 3: Product Extraction

| Priority | Module             | File                    | Purpose                          |
| -------- | ------------------ | ----------------------- | -------------------------------- |
| ğŸ”´       | JSON-CSS extractor | `json_css_extractor.py` | Primary schema-based extractor   |
| ğŸ”´       | DeepSeek extractor | `deepseek_extractor.py` | Fallback extractor from Markdown |
| ğŸ”´       | Schemas            | `product_schemas.py`    | Define Crawl4AI selectors        |

### ğŸŸ¦ Phase 4: Pipeline Orchestration

| Priority | Module           | File          | Purpose                                      |
| -------- | ---------------- | ------------- | -------------------------------------------- |
| ğŸ”´       | Unified pipeline | `pipeline.py` | Run discovery + extraction per roaster       |
| ğŸ”µ       | CLI runner       | `main.py`     | Load CSV input, trigger pipeline, export CSV |

---

## ğŸ’¡ Tips for Implementation

* Use `AsyncWebCrawler` with caching enabled
* Add retry logic (`tenacity`) on network calls
* Markdown fallback should trigger *only* if â‰¥2 critical fields missing (`roast_level`, `bean_type`, `price`, etc.)
* Cache both roaster and product-level outputs as JSON
* Use `slugify(name)` + UUID fallback for unique slugs

---

## ğŸ›  Suggested Commands

```bash
# Basic run
python main.py --input=data/roasters.csv

# Force cache refresh
python main.py --input=data/roasters.csv --refresh

# Export CSVs
python main.py --input=data/roasters.csv --export=coffees.csv
```

---

## ğŸ“Œ Next Steps

* [ ] Finalize `schemas/product_schemas.py`
* [ ] Complete `json_css_extractor.py` and fallback logic
* [ ] Validate pipeline on 5 real roasters
* [ ] Add logging + DeepSeek usage counter
* [ ] Schedule CI runs via GitHub Actions (optional)

---